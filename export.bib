@techReport{Zhang2022,
   abstract = {Self-attention based Transformer models have demonstrated impressive results for image classification and object detection, and more recently for video understanding. Inspired by this success, we investigate the application of Transformer networks for temporal action lo-calization in videos. To this end, we present ActionFormer-a simple yet powerful model to identify actions in time and recognize their categories in a single shot, without using action proposals or relying on pre-defined anchor windows. ActionFormer combines a multiscale feature representation with local self-attention, and uses a light-weighted decoder to classify every moment in time and estimate the corresponding action boundaries. We show that this orchestrated design results in major improvements upon prior works. Without bells and whistles, Ac-tionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outper-forming the best prior model by 14.1 absolute percentage points. Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.6% average mAP) and EPIC-Kitchens 100 (+13.5% average mAP over prior works). Our code is available at https://github.com/happyharrycn/ actionformer_release.},
   author = {Chen-Lin Zhang and Jianxin Wu and Yin Li},
   keywords = {action recognition,egocentric vision,temporal action localization,video understanding,vision transformers},
   title = {ActionFormer: Localizing Moments of Actions with Transformers},
   url = {https://github.com/happyharrycn/},
   year = {2022}
}
@techReport{Wang2024,
   abstract = {We introduce InternVideo2, a new family of video foundation models (ViFM) that achieve the state-of-the-art results in video recognition, video-text tasks, and video-centric dialogue. Our core design is a progressive training approach that unifies the masked video modeling, crossmodal contrastive learning, and next token prediction, scaling up the video encoder size to 6B parameters. At the data level, we prioritize spatiotemporal consistency by semantically segmenting videos and generating video-audio-speech captions. This improves the alignment between video and text. Through extensive experiments, we validate our designs and demonstrate superior performance on over 60 video and audio tasks. Notably, our model outperforms others on various video-related dialogue and long video understanding benchmarks, highlighting its ability to reason and comprehend longer contexts.},
   author = {Yi Wang and Kunchang Li and Xinhao Li and Jiashuo Yu and Yinan He and Chenting Wang and Guo Chen and Baoqi Pei and Ziang Yan and Rongkun Zheng and Jilan Xu and Zun Wang and Yansong Shi and Tianxiang Jiang and Songze Li and Hongjie Zhang and Yifei Huang and Yu Qiao and Yali Wang †3 and Limin Wang},
   title = {INTERNVIDEO2: SCALING FOUNDATION MODELS FOR MULTIMODAL VIDEO UNDERSTANDING Transferable Video(-Text) Representation Temporal Commonsense Reasoning Support for Long Videos Visual inputs Inaccessible in eval},
   url = {https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2},
   year = {2024}
}
@book{DouglasAdams1979,
   author = {Douglas Adams},
   title = {The Hitchhiker’s Guide to the Galaxy by Douglas Adams | Goodreads},
   url = {https://www.goodreads.com/book/show/11.The_Hitchhiker_s_Guide_to_the_Galaxy},
   year = {1979}
}
